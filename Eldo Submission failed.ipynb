{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/gist/alexandralht0413/33f5612a9ee96406bacf95497ed39e9d/assignment-4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hRiOZ72N630"
   },
   "source": [
    "# Assignment 4\n",
    "## Understaning scaling of linear algebra operations on Apache Spark using Apache SystemML\n",
    "\n",
    "In this assignment we want you to understand how to scale linear algebra operations from a single machine to multiple machines, memory and CPU cores using Apache SystemML. Therefore we want you to understand how to migrate from a numpy program to a SystemML DML program. Don't worry. We will give you a lot of hints. Finally, you won't need this knowledge anyways if you are sticking to Keras only, but once you go beyond that point you'll be happy to see what's going on behind the scenes.\n",
    "\n",
    "So the first thing we need to ensure is that we are on the latest version of SystemML, which is 1.2.0:\n",
    "\n",
    "The steps are:\n",
    "- pip install\n",
    "- start execution at the cell with the version - check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rfxW6e8SN637"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n",
    "if ('sc' in locals() or 'sc' in globals()):\n",
    "    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1SIDNEmoN638",
    "outputId": "2c1b39fa-ba25-4ce2-a79f-21f506d6fbf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.3.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (3.3.0)\r\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from pyspark==3.3.0) (0.10.9.5)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==3.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iOjlZ2v0N63-",
    "outputId": "1eef88ea-1137-4b2e-ed7e-0f2739494dc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting systemds==2.2.1\n",
      "  Downloading systemds-2.2.1-py3-none-any.whl (50.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: py4j>=0.10.9 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from systemds==2.2.1) (0.10.9.5)\n",
      "Requirement already satisfied: requests>=2.24.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from systemds==2.2.1) (2.31.0)\n",
      "Requirement already satisfied: pandas>=1.2.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from systemds==2.2.1) (1.4.3)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from systemds==2.2.1) (1.23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from pandas>=1.2.2->systemds==2.2.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from pandas>=1.2.2->systemds==2.2.1) (2023.3.post1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests>=2.24.0->systemds==2.2.1) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests>=2.24.0->systemds==2.2.1) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests>=2.24.0->systemds==2.2.1) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests>=2.24.0->systemds==2.2.1) (1.26.16)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=1.2.2->systemds==2.2.1) (1.16.0)\n",
      "Installing collected packages: systemds\n",
      "Successfully installed systemds-2.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install systemds==2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Srp7kxh1N63_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/08 01:52:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nGBBXeAJN63_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "u = np.random.rand(1000,10000)\n",
    "s = np.random.rand(10000,1000)\n",
    "w = np.random.rand(1000,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcX9itv3N64A"
   },
   "source": [
    "Now we implement a short one-liner to define a very simple linear algebra operation\n",
    "\n",
    "In case you are unfamiliar with matrxi-matrix multiplication: https://en.wikipedia.org/wiki/Matrix_multiplication\n",
    "\n",
    "sum(U' * (W . (U * S)))\n",
    "\n",
    "\n",
    "| Legend        |            |   \n",
    "| ------------- |-------------|\n",
    "| '      | transpose of a matrix |\n",
    "| * | matrix-matrix multiplication      |  \n",
    "| . | scalar multiplication      |   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KRYY_lPDN64A",
    "outputId": "e05272d0-8dfa-4bbe-de8c-00885d8a5c62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2109904289245605\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "res = np.sum(u.T.dot(w * u.dot(s)))\n",
    "print (time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsnDWqkPN64A"
   },
   "source": [
    "As you can see this executes perfectly fine. Note that this is even a very efficient execution because numpy uses a C/C++ backend which is known for it's performance. But what happens if U, S or W get such big that the available main memory cannot cope with it? Let's give it a try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfhzrYA2N64B"
   },
   "outputs": [],
   "source": [
    "#u = np.random.rand(10000,100000)\n",
    "#s = np.random.rand(100000,10000)\n",
    "#w = np.random.rand(10000,10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Uvef7aeN64B"
   },
   "source": [
    "After a short while you should see a memory error. This is because the operating system process was not able to allocate enough memory for storing the numpy array on the heap. Now it's time to re-implement the very same operations in SystemDS, and this is your task. Just replace all ###your_code_goes_here sections with proper code, please consider the following table which contains all syntax you need:\n",
    "\n",
    "| Syntax        |            |   \n",
    "| ------------- |-------------|\n",
    "| M.t()      | transpose of a matrix, where M is the matrix |\n",
    "| `M.__matmul__(N)` | matrix-matrix multiplication between M and N      |  \n",
    "| M * N | scalar multiplication between matrix M and N     |   \n",
    "\n",
    "\n",
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIafyYGJN64B"
   },
   "source": [
    "We use SystemDSContext to interface with Apache SystemDS (formerly SystemML). Note that we passed a SparkSession object as parameter to SystemDSContext so now it knows how how to talk to the Apache Spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6244508396425750.0\n"
     ]
    }
   ],
   "source": [
    "# Option 1 Eldo - FAILED, USING (U.t().__matmul__(W.__matmul__(U.__matmul__(S)))).sum()  \n",
    "from systemds.context import SystemDSContext\n",
    "with SystemDSContext(spark) as sds:\n",
    "    # Now we create some large random matrices to have numpy and SystemDS crunch on it\n",
    "    U = sds.rand(rows=1000,cols=10000)\n",
    "    S = sds.rand(rows=10000,cols=1000)\n",
    "    W = sds.rand(rows=1000,cols=1000)\n",
    "    # res = (U.###your_code_goes_here.###your_code_goes_here((W * (U.###your_code_goes_here(S))))).sum()\n",
    "    res = (U.t().__matmul__(W.__matmul__(U.__matmul__(S)))).sum()           \n",
    "    print(res.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6251130091127.43\n"
     ]
    }
   ],
   "source": [
    "# Option 2 Eldo - FAILED, USING Matrix-Matrix Multiplication , Transpose & Scalar Multiplication\n",
    "from systemds.context import SystemDSContext\n",
    "\n",
    "with SystemDSContext(spark) as sds:\n",
    "    # Create large random matrices\n",
    "    U = sds.rand(rows=1000, cols=10000)\n",
    "    S = sds.rand(rows=10000, cols=1000)\n",
    "    W = sds.rand(rows=1000, cols=1000)\n",
    "\n",
    "    # Define the expression\n",
    "    part1 = U.__matmul__(S)  # Matrix-matrix multiplication\n",
    "    part2 = W * part1  # Scalar multiplication\n",
    "    res = U.t().__matmul__(part2)  # Transpose and matrix-matrix multiplication\n",
    "\n",
    "    # Compute the result\n",
    "    result = res.sum()\n",
    "\n",
    "    # Print the result\n",
    "    print(result.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250550179480467.0\n"
     ]
    }
   ],
   "source": [
    "# Option 3 Eldo - FAILED, USING U.t() @ (W @ (U @ S))\n",
    "from systemds.context import SystemDSContext\n",
    "import numpy as np\n",
    "with SystemDSContext(spark) as sds:\n",
    "    # Create large random matrices\n",
    "    U = sds.rand(rows=1000, cols=10000)\n",
    "    S = sds.rand(rows=10000, cols=1000)\n",
    "    W = sds.rand(rows=1000, cols=1000)\n",
    "    \n",
    "    # Define the expression\n",
    "    expr = U.t() @ (W @ (U @ S))\n",
    "   \n",
    "    # Compute the result\n",
    "    res = expr.sum()\n",
    "    \n",
    "    # Print the result\n",
    "    print(res.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6256639508741154.0\n"
     ]
    }
   ],
   "source": [
    "# Option 4 Eldo - FAILED, USING np.matmul(U_np.T, np.matmul(W_np, np.matmul(U_np, S_np))).sum()\n",
    "from systemds.context import SystemDSContext\n",
    "import numpy as np\n",
    "with SystemDSContext(spark) as sds:\n",
    "    # Create large random matrices\n",
    "    U = sds.rand(rows=1000, cols=10000)\n",
    "    S = sds.rand(rows=10000, cols=1000)\n",
    "    W = sds.rand(rows=1000, cols=1000)\n",
    "\n",
    "    # Convert U, S, and W to numpy arrays\n",
    "    U_np = U.compute()\n",
    "    S_np = S.compute()\n",
    "    W_np = W.compute()\n",
    "\n",
    "    # Perform matrix multiplications using numpy functions\n",
    "    res = np.matmul(U_np.T, np.matmul(W_np, np.matmul(U_np, S_np))).sum()\n",
    "\n",
    "    # Print the result\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6253453282248347.0\n"
     ]
    }
   ],
   "source": [
    "# Option 5 Eldo - FAILED, USING (U @ (S @ W @ U)).sum() \n",
    "from systemds.context import SystemDSContext    \n",
    "with SystemDSContext(spark) as sds:\n",
    "    U = sds.rand(rows=1000,cols=10000)\n",
    "    S = sds.rand(rows=10000,cols=1000)\n",
    "    W = sds.rand(rows=1000,cols=1000)\n",
    "    res = (U @ (S @ W @ U)).sum() \n",
    "\n",
    "    print (res.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6249622544674791.0\n"
     ]
    }
   ],
   "source": [
    "#Option 6 Eldo - FAILED, using  (U.t() @ (W @ (U @ S))).sum()\n",
    "from systemds.context import SystemDSContext\n",
    "with SystemDSContext(spark) as sds:\n",
    "\n",
    "    # Now we create some large random matrices to have numpy and SystemDS crunch on it\n",
    "\n",
    "    U = sds.rand(rows=1000, cols=10000)\n",
    "    S = sds.rand(rows=10000, cols=1000)\n",
    "    W = sds.rand(rows=1000, cols=1000)\n",
    "\n",
    "    expr = (U.t() @ (W @ (U @ S))).sum()\n",
    "    # Compute the result\n",
    "    res = expr\n",
    "    # Print the result\n",
    "    print(res.compute()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option 7 Eldo - FAILED OUT OF MEMORY\n",
    "from systemds.context import SystemDSContext\n",
    "\n",
    "with SystemDSContext(spark) as sds:\n",
    "    # Create large random matrices\n",
    "    U = sds.rand(rows=1000, cols=10000)\n",
    "    S = sds.rand(rows=10000, cols=1000)\n",
    "    W = sds.rand(rows=1000, cols=1000)\n",
    "    \n",
    "    # Define the expression\n",
    "    res = sum(U.t().__matmul__(W * (U.__matmul__(S))))\n",
    "    \n",
    "    # Print the result\n",
    "    print(res.compute())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_s9GECJTN64C"
   },
   "source": [
    "In order to show you the advantage of SystemML over numpy we've blown up the sizes of the matrices. Unfortunately, on a 1-2 worker Spark cluster it takes quite some time to complete. Therefore we've stripped down the example to smaller matrices below, but we've kept the code, just in case you are curious to check it out. But you might want to use some more workers which you easily can configure in the environment settings of the project within Watson Studio. Just be aware that you're currently limited to free 50 capacity unit hours per month wich are consumed by the additional workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nin6RIPN64C"
   },
   "source": [
    "To get consistent results we switch from a random matrix initialization to something deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuYuTG2oN64C"
   },
   "source": [
    "If everything runs fine you should get *6252492444241.075* as result (or something in that bullpark). Feel free to submit your solutionto the grader now!\n",
    "\n",
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5DZ8mVYON64D",
    "outputId": "2fcc906b-f3e8-46cf-abbc-f64ea8e108e2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-08 00:50:35--  https://raw.githubusercontent.com/romeokienzler/developerWorks/master/coursera/ai/rklib.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2289 (2.2K) [text/plain]\n",
      "Saving to: ‘rklib.py’\n",
      "\n",
      "rklib.py            100%[===================>]   2.24K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-11-08 00:50:35 (27.7 MB/s) - ‘rklib.py’ saved [2289/2289]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -f rklib.py\n",
    "!wget https://raw.githubusercontent.com/romeokienzler/developerWorks/master/coursera/ai/rklib.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rklib\n",
      "  Downloading rklib-0.1.0-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from rklib) (2.31.0)\n",
      "Collecting green\n",
      "  Downloading green-3.4.3.tar.gz (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from rklib) (6.0)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting coverage\n",
      "  Downloading coverage-7.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.5/227.5 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting unidecode\n",
      "  Downloading Unidecode-1.3.7-py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: lxml in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from green->rklib) (4.9.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->rklib) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->rklib) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->rklib) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->rklib) (1.26.16)\n",
      "Building wheels for collected packages: green\n",
      "  Building wheel for green (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for green: filename=green-3.4.3-py3-none-any.whl size=75546 sha256=6cbf585ee3fb96be35c2b535669410acd9b41893ca111d983b91b5420b8dd028\n",
      "  Stored in directory: /tmp/wsuser/.cache/pip/wheels/08/83/a5/b8520c7409232a2b77351f4cbba7e05d41d3005ad5ffd4998f\n",
      "Successfully built green\n",
      "Installing collected packages: unidecode, coverage, colorama, green, rklib\n",
      "Successfully installed colorama-0.4.6 coverage-7.3.2 green-3.4.3 rklib-0.1.0 unidecode-1.3.7\n"
     ]
    }
   ],
   "source": [
    "!pip install rklib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rklib import submit\n",
    "key = \"esRk7vn-Eeej-BLTuYzd0g\"\n",
    "part = \"fUxc8\"\n",
    "\n",
    "email = 'eldoma@gmail.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l5ZEP3gqN64D",
    "outputId": "aa7768f5-ed3b-47f1-9eaa-db24a335b185"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission successful, please check on the coursera grader page for the status\n",
      "-------------------------\n",
      "{\"elements\":[{\"itemId\":\"P1p3F\",\"id\":\"tE4j0qhMEeecqgpT6QjMdA~P1p3F~1H1qNn3cEe66DRLtNxmX4Q\",\"courseId\":\"tE4j0qhMEeecqgpT6QjMdA\"}],\"paging\":{},\"linked\":{}}\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "part = \"fUxc8\"\n",
    "token = \"ba7vmJwZpIwOKBU5\" ###your_code_goes_here #you can obtain it from the grader page on Coursera (have a look here if you need more information on how to obtain the token https://youtu.be/GcDo0Rwe06U?t=276)\n",
    "submit(email, token, key, part, [part], str(res).replace('\\n','x'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
